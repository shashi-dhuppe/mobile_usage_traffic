{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import sys\n",
    "findspark.init('/usr/local/spark/')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "import time\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml import feature, regression, Pipeline\n",
    "\n",
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName('data-model').getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import classification\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictionAndLabels):\n",
    "    log = {}\n",
    "\n",
    "    # Show Validation Score (AUROC)\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "    log['AUROC'] = \"%f\" % evaluator.evaluate(predictionAndLabels)    \n",
    "    print(\"Area under ROC = {}\".format(log['AUROC']))\n",
    "\n",
    "    # Show Validation Score (AUPR)\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
    "    log['AUPR'] = \"%f\" % evaluator.evaluate(predictionAndLabels)\n",
    "    print(\"Area under PR = {}\".format(log['AUPR']))\n",
    "\n",
    "    # Metrics\n",
    "    predictionRDD = predictionAndLabels.select(['label', 'prediction']) \\\n",
    "                            .rdd.map(lambda line: (line[1], line[0]))\n",
    "    metrics = MulticlassMetrics(predictionRDD)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "    # Overall statistics\n",
    "    log['precision'] = \"%s\" % metrics.precision()\n",
    "    log['recall'] = \"%s\" % metrics.recall()\n",
    "    log['F1 Measure'] = \"%s\" % metrics.fMeasure()\n",
    "    print(\"[Overall]\\tprecision = %s | recall = %s | F1 Measure = %s\" % \\\n",
    "            (log['precision'], log['recall'], log['F1 Measure']))\n",
    "\n",
    "    # Statistics by class\n",
    "    labels = [0.0, 1.0]\n",
    "    for label in sorted(labels):\n",
    "        log[label] = {}\n",
    "        log[label]['precision'] = \"%s\" % metrics.precision(label)\n",
    "        log[label]['recall'] = \"%s\" % metrics.recall(label)\n",
    "        log[label]['F1 Measure'] = \"%s\" % metrics.fMeasure(label, \n",
    "                                                           beta=1.0)\n",
    "        print(\"[Class %s]\\tprecision = %s | recall = %s | F1 Measure = %s\" \\\n",
    "                  % (label, log[label]['precision'], \n",
    "                    log[label]['recall'], log[label]['F1 Measure']))\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spark.read.format('csv').option('header', 'true').load('/user/ananth/spark_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134550"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>label_id</th>\n",
       "      <th>app_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>is_installed</th>\n",
       "      <th>is_active</th>\n",
       "      <th>device_model</th>\n",
       "      <th>phone_brand</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>category_mapped</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>seconds</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4968154927622705128</td>\n",
       "      <td>713</td>\n",
       "      <td>-145658454112781034</td>\n",
       "      <td>4633</td>\n",
       "      <td>116.38</td>\n",
       "      <td>39.96</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>荣耀6 Plus</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>...</td>\n",
       "      <td>China</td>\n",
       "      <td>industry</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>50-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4968154927622705128</td>\n",
       "      <td>704</td>\n",
       "      <td>-145658454112781034</td>\n",
       "      <td>4633</td>\n",
       "      <td>116.38</td>\n",
       "      <td>39.96</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>荣耀6 Plus</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>...</td>\n",
       "      <td>China</td>\n",
       "      <td>industry</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>50-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4968154927622705128</td>\n",
       "      <td>548</td>\n",
       "      <td>-145658454112781034</td>\n",
       "      <td>4633</td>\n",
       "      <td>116.38</td>\n",
       "      <td>39.96</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>荣耀6 Plus</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>...</td>\n",
       "      <td>China</td>\n",
       "      <td>industry</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>morning</td>\n",
       "      <td>50-59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              device_id label_id               app_id event_id longitude  \\\n",
       "0  -4968154927622705128      713  -145658454112781034     4633    116.38   \n",
       "1  -4968154927622705128      704  -145658454112781034     4633    116.38   \n",
       "2  -4968154927622705128      548  -145658454112781034     4633    116.38   \n",
       "\n",
       "  latitude is_installed is_active device_model phone_brand  ... country  \\\n",
       "0    39.96            1         0     荣耀6 Plus      Huawei  ...   China   \n",
       "1    39.96            1         0     荣耀6 Plus      Huawei  ...   China   \n",
       "2    39.96            1         0     荣耀6 Plus      Huawei  ...   China   \n",
       "\n",
       "  category_mapped  year month day hour minute seconds time_of_day age_group  \n",
       "0        industry  2016     5   1    7     48       6     morning     50-59  \n",
       "1        industry  2016     5   1    7     48       6     morning     50-59  \n",
       "2        industry  2016     5   1    7     48       6     morning     50-59  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.select('town').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, test = train_data.randomSplit([0.7, 0.2, 0.1], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = ['device_id', 'app_id', 'label_id', 'event_id', 'longitude', 'latitude']\n",
    "int_columns = ['is_active', 'age', 'is_installed', 'day', 'hour', 'minute', 'seconds']\n",
    "string_columns = ['gender', 'phone_brand', 'device_model', 'town', 'country', 'category_mapped', 'time_of_day', 'age_group']\n",
    "\n",
    "training = training.select(*(col(c).cast(\"float\").alias(c) for c in float_columns), \\\n",
    "                                                 *(col(c).cast(\"int\").alias(c) for c in int_columns), \\\n",
    "                                                 *(col(c).alias(c) for c in string_columns))\n",
    "\n",
    "validation = validation.select(*(col(c).cast(\"float\").alias(c) for c in float_columns), \\\n",
    "                                                 *(col(c).cast(\"int\").alias(c) for c in int_columns), \\\n",
    "                                                 *(col(c).alias(c) for c in string_columns))\n",
    "\n",
    "test = test.select(*(col(c).cast(\"float\").alias(c) for c in float_columns), \\\n",
    "                                                 *(col(c).cast(\"int\").alias(c) for c in int_columns), \\\n",
    "                                                 *(col(c).alias(c) for c in string_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94302"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PCA for town\n",
    "\n",
    "# gender_indexer = feature.StringIndexer(inputCol=\"gender\", outputCol=\"gender_label\",handleInvalid='skip')\n",
    "# category_indexer = feature.StringIndexer(inputCol='category_mapped', outputCol='category_encoded',handleInvalid='skip')\n",
    "# phone_brand_indexer = feature.StringIndexer(inputCol='phone_brand', outputCol='phone_brand_encoded',handleInvalid='skip')\n",
    "# is_active_indexer = feature.StringIndexer(inputCol='is_active', outputCol='is_active_encoded',handleInvalid='skip')\n",
    "# device_model_indexer = feature.StringIndexer(inputCol='device_model', outputCol='device_model_encoded',handleInvalid='skip')\n",
    "# town_indexer = feature.StringIndexer(inputCol='town', outputCol='town_encoded',handleInvalid='skip')\n",
    "# country_indexer = feature.StringIndexer(inputCol='country', outputCol='country_encoded',handleInvalid='skip')\n",
    "# time_of_day_indexer = feature.StringIndexer(inputCol='time_of_day', outputCol='time_of_day_encoded',handleInvalid='skip')\n",
    "# age_group_indexer = feature.StringIndexer(inputCol='age_group', outputCol='age_group_encoded',handleInvalid='skip')\n",
    "# #area_cluster_id_indexer = feature.StringIndexer(inputCol='area_cluster_id', outputCol='area_cluster_id_encoded',handleInvalid='skip')\n",
    "\n",
    "# vector_assembler = feature.VectorAssembler(inputCols=['device_id', 'app_id', 'label_id', 'event_id', 'is_active',\\\n",
    "#                                                       'device_model_encoded', 'phone_brand_encoded', 'gender_label', 'country_encoded',\\\n",
    "#                                                       'time_of_day_encoded', 'age_group_encoded', 'category_encoded'],\n",
    "#                                         outputCol='features')\n",
    "# sc = feature.StandardScaler(inputCol='features',outputCol='sfeatures')\n",
    "\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol='gender_label')\n",
    "\n",
    "# pipe_prep_location=Pipeline(stages=[gender_indexer, category_indexer,phone_brand_indexer, is_active_indexer, device_model_indexer,\\\n",
    "#                            town_indexer, country_indexer, time_of_day_indexer, age_group_indexer, \\\n",
    "#                            vector_assembler, sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# # df_kmeans.show()\n",
    "# df_kmeans = pipe_prep_location.fit(training).transform(training)\n",
    "# cost = np.zeros(30)\n",
    "# for k in range(2,30):\n",
    "#     kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"sfeatures\")\n",
    "#     model = kmeans.fit(df_kmeans.sample(False,0.1, seed=42))\n",
    "#     cost[k] = model.computeCost(df_kmeans) # requires Spark 2.0 or later\n",
    "    \n",
    "# fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "# ax.plot(range(2,30),cost[2:30])\n",
    "# ax.set_xlabel('k')\n",
    "# ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.clustering import KMeans\n",
    "# k = 30\n",
    "# df_kmeans = pipe_prep_location.fit(train_data).transform(train_data)\n",
    "# kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"sfeatures\")\n",
    "# model = kmeans.fit(df_kmeans)\n",
    "# centers = model.clusterCenters()\n",
    "\n",
    "# print(\"Cluster Centers: \")\n",
    "# for center in centers:\n",
    "#     print(center)\n",
    "\n",
    "# new_data = model.transform(df_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender Pipeline\n",
    "gender_indexer = feature.StringIndexer(inputCol=\"gender\", outputCol=\"label\",handleInvalid='skip')\n",
    "category_indexer = feature.StringIndexer(inputCol='category_mapped', outputCol='category_encoded',handleInvalid='skip')\n",
    "phone_brand_indexer = feature.StringIndexer(inputCol='phone_brand', outputCol='phone_brand_encoded',handleInvalid='skip')\n",
    "is_active_indexer = feature.StringIndexer(inputCol='is_active', outputCol='is_active_encoded',handleInvalid='skip')\n",
    "device_model_indexer = feature.StringIndexer(inputCol='device_model', outputCol='device_model_encoded',handleInvalid='skip')\n",
    "town_indexer = feature.StringIndexer(inputCol='town', outputCol='town_encoded',handleInvalid='skip')\n",
    "country_indexer = feature.StringIndexer(inputCol='country', outputCol='country_encoded',handleInvalid='skip')\n",
    "time_of_day_indexer = feature.StringIndexer(inputCol='time_of_day', outputCol='time_of_day_encoded',handleInvalid='skip')\n",
    "age_group_indexer = feature.StringIndexer(inputCol='age_group', outputCol='age_group_encoded',handleInvalid='skip')\n",
    "#area_cluster_id_indexer = feature.StringIndexer(inputCol='area_cluster_id', outputCol='area_cluster_id_encoded',handleInvalid='skip')\n",
    "\n",
    "vector_assembler = feature.VectorAssembler(inputCols=['device_id', 'app_id', 'label_id', 'event_id', 'is_active',\\\n",
    "                                                      'device_model_encoded', 'phone_brand_encoded', 'town_encoded', 'country_encoded',\\\n",
    "                                                      'time_of_day_encoded', 'age_group_encoded', 'category_encoded'],\n",
    "                                        outputCol='features')\n",
    "sc = feature.StandardScaler(inputCol='features',outputCol='sfeatures')\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "\n",
    "pipe_prep=Pipeline(stages=[gender_indexer, category_indexer,phone_brand_indexer, is_active_indexer, device_model_indexer,\\\n",
    "                           town_indexer, country_indexer, time_of_day_indexer, age_group_indexer, \\\n",
    "                           vector_assembler, sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=feature.PCA(k=2, inputCol='sfeatures', outputCol='pfeat')\n",
    "\n",
    "pipe_pca=Pipeline(stages=[pipe_prep,pca]).fit(training)\n",
    "\n",
    "pca_mod=pipe_pca.transform(training)\n",
    "\n",
    "feat=train_data.columns\n",
    "actfeat=['device_id', 'app_id', 'label_id', 'event_id', 'is_active',\\\n",
    "                                                      'device_model_encoded', 'phone_brand_encoded', 'town_encoded', 'country_encoded',\\\n",
    "                                                      'time_of_day_encoded', 'age_group_encoded', 'category_encoded']\n",
    "\n",
    "# feat\n",
    "\n",
    "# actfeat=pca_mod.columns\n",
    "\n",
    "pca=pipe_pca.stages[1].pc.toArray()\n",
    "\n",
    "pc1_df=pd.DataFrame([pca[:, 0],actfeat]).T.rename(columns={0:'pc1',1:'abs_loadings'})\n",
    "pc2_df=pd.DataFrame([pca[:, 1],actfeat]).T.rename(columns={0:'pc2',1:'abs_loadings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>abs_loadings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.678323</td>\n",
       "      <td>phone_brand_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.672741</td>\n",
       "      <td>device_model_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15323</td>\n",
       "      <td>is_active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.141971</td>\n",
       "      <td>country_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.138975</td>\n",
       "      <td>age_group_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.126475</td>\n",
       "      <td>town_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0832806</td>\n",
       "      <td>device_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0248684</td>\n",
       "      <td>app_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0239107</td>\n",
       "      <td>category_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0110473</td>\n",
       "      <td>time_of_day_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00808621</td>\n",
       "      <td>event_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00623352</td>\n",
       "      <td>label_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pc1          abs_loadings\n",
       "6     0.678323   phone_brand_encoded\n",
       "5     0.672741  device_model_encoded\n",
       "4      0.15323             is_active\n",
       "8     0.141971       country_encoded\n",
       "10    0.138975     age_group_encoded\n",
       "7     0.126475          town_encoded\n",
       "0    0.0832806             device_id\n",
       "1    0.0248684                app_id\n",
       "11   0.0239107      category_encoded\n",
       "9    0.0110473   time_of_day_encoded\n",
       "3   0.00808621              event_id\n",
       "2   0.00623352              label_id"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc1_df.pc1=pc1_df.pc1.abs()\n",
    "\n",
    "pc1_df.sort_values(by=['pc1'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc2</th>\n",
       "      <th>abs_loadings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.702398</td>\n",
       "      <td>label_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.695127</td>\n",
       "      <td>category_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.115651</td>\n",
       "      <td>is_active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0800999</td>\n",
       "      <td>town_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0354481</td>\n",
       "      <td>age_group_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0335258</td>\n",
       "      <td>app_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0237315</td>\n",
       "      <td>time_of_day_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0206697</td>\n",
       "      <td>country_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0113723</td>\n",
       "      <td>device_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00748714</td>\n",
       "      <td>event_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00708659</td>\n",
       "      <td>phone_brand_encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00613241</td>\n",
       "      <td>device_model_encoded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pc2          abs_loadings\n",
       "2     0.702398              label_id\n",
       "11    0.695127      category_encoded\n",
       "4     0.115651             is_active\n",
       "7    0.0800999          town_encoded\n",
       "10   0.0354481     age_group_encoded\n",
       "1    0.0335258                app_id\n",
       "9    0.0237315   time_of_day_encoded\n",
       "8    0.0206697       country_encoded\n",
       "0    0.0113723             device_id\n",
       "3   0.00748714              event_id\n",
       "6   0.00708659   phone_brand_encoded\n",
       "5   0.00613241  device_model_encoded"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc2_df.pc2=pc2_df.pc2.abs()\n",
    "\n",
    "pc2_df.sort_values(by=['pc2'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6179698897928729"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic with default or no parameters\n",
    "\n",
    "logistic = classification.LogisticRegression(labelCol='label', featuresCol='sfeatures')\n",
    "\n",
    "lr_pipe = Pipeline(stages=[pipe_prep, logistic]).fit(training)\n",
    "\n",
    "result1=evaluator.evaluate(lr_pipe.transform(test))\n",
    "\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.617970\n",
      "Area under PR = 0.278325\n",
      "[[10674.    16.]\n",
      " [ 2609.     0.]]\n",
      "[Overall]\tprecision = 0.8026167381006091 | recall = 0.8026167381006091 | F1 Measure = 0.8026167381006091\n",
      "[Class 0.0]\tprecision = 0.8035835278175111 | recall = 0.9985032740879326 | F1 Measure = 0.890501814541359\n",
      "[Class 1.0]\tprecision = 0.0 | recall = 0.0 | F1 Measure = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUROC': '0.617970',\n",
       " 'AUPR': '0.278325',\n",
       " 'precision': '0.8026167381006091',\n",
       " 'recall': '0.8026167381006091',\n",
       " 'F1 Measure': '0.8026167381006091',\n",
       " 0.0: {'precision': '0.8035835278175111',\n",
       "  'recall': '0.9985032740879326',\n",
       "  'F1 Measure': '0.890501814541359'},\n",
       " 1.0: {'precision': '0.0', 'recall': '0.0', 'F1 Measure': '0.0'}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lr_pipe.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC Curve: 0.6180\n",
      "Area under PR Curve: 0.2783\n"
     ]
    }
   ],
   "source": [
    "auroc = evaluator.evaluate(lr_pipe.transform(test), {evaluator.metricName: \"areaUnderROC\"})\n",
    "auprc = evaluator.evaluate(lr_pipe.transform(test), {evaluator.metricName: \"areaUnderPR\"})\n",
    "print(\"Area under ROC Curve: {:.4f}\".format(auroc))\n",
    "print(\"Area under PR Curve: {:.4f}\".format(auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_29c6309518dc', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0,\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto',\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='featuresCol', doc='features column name'): 'sfeatures',\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='labelCol', doc='label column name'): 'gender_label',\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='maxIter', doc='maximum number of iterations (>= 0)'): 100,\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='regParam', doc='regularization parameter (>= 0)'): 0.01,\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5,\n",
       " Param(parent='LogisticRegression_29c6309518dc', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr_pipe = Pipeline(stages=[pipe_prep, logistic])\n",
    "\n",
    "lr_param = ParamGridBuilder() \\\n",
    "    .addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(logistic.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName='areaUnderROC')\n",
    "#evaluator = RegressionEvaluator(labelCol=\"town_encoded\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol = 'town_encoded', predictionCol='prediction', metricName = 'accuracy')\n",
    "crossval = CrossValidator(estimator=lr_pipe,\n",
    "                         estimatorParamMaps=lr_param,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)\n",
    "\n",
    "cvmodel = crossval.fit(validation)\n",
    "cvmodel.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7248086612604546"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF\n",
    "\n",
    "rf=classification.RandomForestClassifier(labelCol='label', featuresCol='sfeatures')\n",
    "\n",
    "rf_pipe=Pipeline(stages=[pipe_prep,rf]).fit(training)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'label', metricName ='areaUnderROC')\n",
    "\n",
    "resultrf4=evaluator.evaluate(rf_pipe.transform(validation))\n",
    "\n",
    "resultrf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_084483c7c992', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto',\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='featuresCol', doc='features column name'): 'sfeatures',\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='labelCol', doc='label column name'): 'gender_label',\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 20,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='numTrees', doc='Number of trees to train (>= 1)'): 20,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='seed', doc='random seed'): -8746918203667937428,\n",
       " Param(parent='RandomForestClassifier_084483c7c992', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf_pipe=Pipeline(stages=[pipe_prep,rf])\n",
    "\n",
    "rfParam = ParamGridBuilder() \\\n",
    ".addGrid(rf.maxDepth, [4, 6, 8, 10]) \\\n",
    ".addGrid(rf.maxBins, [5, 10, 20]) \\\n",
    ".addGrid(rf.impurity, [\"gini\"]) \\\n",
    ".build()\n",
    "\n",
    "# gbtParam = (ParamGridBuilder()\n",
    "#              .addGrid(gbt.maxDepth, [4, 6, 8, 10])\n",
    "#              .addGrid(gbt.maxBins, [5, 10, 20, 40])\n",
    "#              .addGrid(gbt.maxIter, [5, 10, 15])\n",
    "#              .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"gender_label\", metricName='areaUnderROC')\n",
    "#evaluator = RegressionEvaluator(labelCol=\"town_encoded\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol = 'town_encoded', predictionCol='prediction', metricName = 'accuracy')\n",
    "crossval = CrossValidator(estimator=rf_pipe,\n",
    "                         estimatorParamMaps=rfParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)\n",
    "\n",
    "#cv = CrossValidator(estimator=random_forest_pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvmodel = crossval.fit(validation)\n",
    "cvmodel.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "rf=classification.RandomForestClassifier(labelCol='label', featuresCol='sfeatures', \\\n",
    "                                         maxDepth=10, maxBins=20, numTrees=20)\n",
    "\n",
    "rf_pipe=Pipeline(stages=[pipe_prep,rf]).fit(training)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName='areaUnderROC')\n",
    "resultrf4=evaluator.evaluate(rf_pipe.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.908114\n",
      "Area under PR = 0.813277\n",
      "[[10675.    15.]\n",
      " [ 1748.   861.]]\n",
      "[Overall]\tprecision = 0.86743364162719 | recall = 0.86743364162719 | F1 Measure = 0.86743364162719\n",
      "[Class 0.0]\tprecision = 0.8592932463978105 | recall = 0.9985968194574368 | F1 Measure = 0.9237225803660278\n",
      "[Class 1.0]\tprecision = 0.9828767123287672 | recall = 0.33001149865848983 | F1 Measure = 0.4941176470588235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUROC': '0.908114',\n",
       " 'AUPR': '0.813277',\n",
       " 'precision': '0.86743364162719',\n",
       " 'recall': '0.86743364162719',\n",
       " 'F1 Measure': '0.86743364162719',\n",
       " 0.0: {'precision': '0.8592932463978105',\n",
       "  'recall': '0.9985968194574368',\n",
       "  'F1 Measure': '0.9237225803660278'},\n",
       " 1.0: {'precision': '0.9828767123287672',\n",
       "  'recall': '0.33001149865848983',\n",
       "  'F1 Measure': '0.4941176470588235'}}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(rf_pipe.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o116529.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/ananth/rf/metadata already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-ffd9aa607343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user/ananth/rf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshouldOverwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handleOverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36msaveImpl\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mPipelineSharedReadWrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidateStages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mPipelineSharedReadWrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36msaveImpl\u001b[0;34m(instance, stages, sc, path)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mstageUids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mjsonParams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'stageUids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstageUids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'language'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Python'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mDefaultParamsWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjsonParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mstagesDir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stages\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36msaveMetadata\u001b[0;34m(instance, path, sc, extraMetadata, paramMap)\u001b[0m\n\u001b[1;32m    454\u001b[0m                                                                  \u001b[0mextraMetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                                                                  paramMap)\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadataJson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadataPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o116529.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/ananth/rf/metadata already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "rf_pipe.save(\"/user/ananth/rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Success !\n"
     ]
    }
   ],
   "source": [
    "# Random Forest one of the Tree\n",
    "tree_to_json = rf_pipe.stages[-1].trees[0].toDebugString\n",
    "\n",
    "import re\n",
    "\n",
    "d = {\"feature 0\": \"device_id\", \"feature 1\": \"app_id\",\\\n",
    "     \"feature 2\": \"label_id\", \"feature 3\": \"event_id\",\\\n",
    "     \"feature 4\": \"is_active\", \"feature 5\": \"device_model_encoded\",\\\n",
    "    \"feature 6\": \"phone_brand_encoded\", \"feature 7\": \"town_encoded\",\\\n",
    "    \"feature 8\": \"country_encoded\", \"feature 9\": \"time_of_day_encoded\",\\\n",
    "    \"feature 10\": \"age_group_encoded\", \"feature 11\": \"category_encoded\"}\n",
    "\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "tree_to_json = replace_all(tree_to_json, d)\n",
    "\n",
    "# Parser\n",
    "def parse(lines):\n",
    "    block = []\n",
    "    while lines :\n",
    "\n",
    "        if lines[0].startswith('If'):\n",
    "            bl = ' '.join(lines.pop(0).split()[1:]).replace('(', '').replace(')', '')\n",
    "            block.append({'name':bl, 'children':parse(lines)})\n",
    "\n",
    "\n",
    "            if lines[0].startswith('Else'):\n",
    "                be = ' '.join(lines.pop(0).split()[1:]).replace('(', '').replace(')', '')\n",
    "                block.append({'name':be, 'children':parse(lines)})\n",
    "        elif not lines[0].startswith(('If','Else')):\n",
    "            block2 = lines.pop(0)\n",
    "            block.append({'name':block2})\n",
    "        else:\n",
    "            break\t\n",
    "    return block\n",
    "\n",
    "# Convert Tree to JSON\n",
    "def tree_json(tree):\n",
    "    data = []\n",
    "    for line in tree.splitlines() : \n",
    "        if line.strip():\n",
    "            line = line.strip()\n",
    "            data.append(line)\n",
    "        else : break\n",
    "        if not line : break\n",
    "    res = []\n",
    "    res.append({'name':'Root', 'children':parse(data[1:])})\n",
    "    with open('/home/ananth/Decision-Tree-Visualization-Spark/data/sample.json', 'w') as outfile:\n",
    "        json.dump(res[0], outfile)\n",
    "    print ('Conversion Success !')\n",
    "tree_json(tree_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"700\"\n",
       "            src=\"../EurekaTrees/trees/tree1.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2060381710>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('../Mytrees/trees/tree1.html', width=1000, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%javascript` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBT without Cross Validation\n",
    "\n",
    "gbt = classification.GBTClassifier(labelCol='gender_label', featuresCol='sfeatures')\n",
    "gbt_pipe=Pipeline(stages=[pipe_prep,gbt]).fit(training)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"gender_label\", metricName='areaUnderROC')\n",
    "\n",
    "resultrf4=evaluator.evaluate(gbt_pipe.transform(test))\n",
    "\n",
    "resultrf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBT Cross Validation\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "gbt_pipe = Pipeline(stages=[pipe_prep, gbt])\n",
    "\n",
    "gbtParam = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [4, 6, 8, 10])\n",
    "             .addGrid(gbt.maxBins, [5, 10, 20])\n",
    "             .addGrid(gbt.maxIter, [2])\n",
    "             .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"gender_label\", metricName='areaUnderROC')\n",
    "#evaluator = RegressionEvaluator(labelCol=\"town_encoded\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol = 'gender_label', predictionCol='prediction', metricName = 'accuracy')\n",
    "crossval = CrossValidator(estimator=gbt_pipe,\n",
    "                         estimatorParamMaps=gbtParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)\n",
    "\n",
    "#cv = CrossValidator(estimator=random_forest_pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "\n",
    "cvmodel = crossval.fit(validation)\n",
    "cvmodel.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = classification.GBTClassifier(labelCol='gender_label', featuresCol='sfeatures', maxDepth=10, maxBins=20, lossType = 'logistic', maxIter=2)\n",
    "gbt_pipe=Pipeline(stages=[pipe_prep,gbt]).fit(training)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"gender_label\", metricName='areaUnderROC')\n",
    "\n",
    "resultrf4=evaluator.evaluate(gbt_pipe.transform(test))\n",
    "\n",
    "resultrf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc = evaluator.evaluate(gbt_pipe.transform(test), {evaluator.metricName: \"areaUnderROC\"})\n",
    "auprc = evaluator.evaluate(gbt_pipe.transform(test), {evaluator.metricName: \"areaUnderPR\"})\n",
    "print(\"Area under ROC Curve: {:.4f}\".format(auroc))\n",
    "print(\"Area under PR Curve: {:.4f}\".format(auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is Active Pipeline\n",
    "gender_indexer = feature.StringIndexer(inputCol=\"gender\", outputCol=\"gender_label\",handleInvalid='skip')\n",
    "category_indexer = feature.StringIndexer(inputCol='category_mapped', outputCol='category_encoded',handleInvalid='skip')\n",
    "phone_brand_indexer = feature.StringIndexer(inputCol='phone_brand', outputCol='phone_brand_encoded',handleInvalid='skip')\n",
    "is_active_indexer = feature.StringIndexer(inputCol='is_active', outputCol='label',handleInvalid='skip')\n",
    "device_model_indexer = feature.StringIndexer(inputCol='device_model', outputCol='device_model_encoded',handleInvalid='skip')\n",
    "town_indexer = feature.StringIndexer(inputCol='town', outputCol='town_encoded',handleInvalid='skip')\n",
    "country_indexer = feature.StringIndexer(inputCol='country', outputCol='country_encoded',handleInvalid='skip')\n",
    "time_of_day_indexer = feature.StringIndexer(inputCol='time_of_day', outputCol='time_of_day_encoded',handleInvalid='skip')\n",
    "age_group_indexer = feature.StringIndexer(inputCol='age_group', outputCol='age_group_encoded',handleInvalid='skip')\n",
    "#area_cluster_id_indexer = feature.StringIndexer(inputCol='area_cluster_id', outputCol='area_cluster_id_encoded',handleInvalid='skip')\n",
    "\n",
    "vector_assembler = feature.VectorAssembler(inputCols=['device_id', 'app_id', 'label_id', 'event_id', 'gender_label',\\\n",
    "                                                      'device_model_encoded', 'phone_brand_encoded', 'town_encoded', 'country_encoded',\\\n",
    "                                                      'time_of_day_encoded', 'age_group_encoded', 'category_encoded'],\n",
    "                                        outputCol='features')\n",
    "sc = feature.StandardScaler(inputCol='features',outputCol='sfeatures')\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label')\n",
    "\n",
    "pipe_prep2=Pipeline(stages=[gender_indexer, category_indexer,phone_brand_indexer, is_active_indexer, device_model_indexer,\\\n",
    "                           town_indexer, country_indexer, time_of_day_indexer, age_group_indexer, \\\n",
    "                           vector_assembler, sc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_65295dcda0c0', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='featuresCol', doc='features column name'): 'sfeatures',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 20,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='numTrees', doc='Number of trees to train (>= 1)'): 20,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='seed', doc='random seed'): -8746918203667937428,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6047458831506994"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic with default or no parameters\n",
    "\n",
    "logistic = classification.LogisticRegression(labelCol='label', featuresCol='sfeatures')\n",
    "\n",
    "lr_pipe = Pipeline(stages=[pipe_prep2, logistic]).fit(training)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'label', metricName ='areaUnderROC')\n",
    "\n",
    "result1=evaluator.evaluate(lr_pipe.transform(test))\n",
    "\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "rf=classification.RandomForestClassifier(labelCol='label', featuresCol='sfeatures', \\\n",
    "                                         maxDepth=10, maxBins=20, numTrees=20)\n",
    "\n",
    "rf_pipe=Pipeline(stages=[pipe_prep2,rf]).fit(training)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName='areaUnderROC')\n",
    "resultrf4=evaluator.evaluate(rf_pipe.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.823505\n",
      "Area under PR = 0.697947\n",
      "[[9123.  180.]\n",
      " [2949. 1047.]]\n",
      "[Overall]\tprecision = 0.764719151815926 | recall = 0.764719151815926 | F1 Measure = 0.764719151815926\n",
      "[Class 0.0]\tprecision = 0.7557157057654076 | recall = 0.980651402773299 | F1 Measure = 0.8536140350877194\n",
      "[Class 1.0]\tprecision = 0.8533007334963325 | recall = 0.262012012012012 | F1 Measure = 0.40091901206203334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUROC': '0.823505',\n",
       " 'AUPR': '0.697947',\n",
       " 'precision': '0.764719151815926',\n",
       " 'recall': '0.764719151815926',\n",
       " 'F1 Measure': '0.764719151815926',\n",
       " 0.0: {'precision': '0.7557157057654076',\n",
       "  'recall': '0.980651402773299',\n",
       "  'F1 Measure': '0.8536140350877194'},\n",
       " 1.0: {'precision': '0.8533007334963325',\n",
       "  'recall': '0.262012012012012',\n",
       "  'F1 Measure': '0.40091901206203334'}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(rf_pipe.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_65295dcda0c0', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'auto',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='featuresCol', doc='features column name'): 'sfeatures',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 20,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='numTrees', doc='Number of trees to train (>= 1)'): 20,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='seed', doc='random seed'): -8746918203667937428,\n",
       " Param(parent='RandomForestClassifier_65295dcda0c0', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf_pipe=Pipeline(stages=[pipe_prep2,rf])\n",
    "\n",
    "rfParam = ParamGridBuilder() \\\n",
    ".addGrid(rf.maxDepth, [4, 6, 8, 10]) \\\n",
    ".addGrid(rf.maxBins, [5, 10, 20]) \\\n",
    ".addGrid(rf.impurity, [\"gini\"]) \\\n",
    ".build()\n",
    "\n",
    "# gbtParam = (ParamGridBuilder()\n",
    "#              .addGrid(gbt.maxDepth, [4, 6, 8, 10])\n",
    "#              .addGrid(gbt.maxBins, [5, 10, 20, 40])\n",
    "#              .addGrid(gbt.maxIter, [5, 10, 15])\n",
    "#              .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName='areaUnderROC')\n",
    "#evaluator = RegressionEvaluator(labelCol=\"town_encoded\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol = 'town_encoded', predictionCol='prediction', metricName = 'accuracy')\n",
    "crossval = CrossValidator(estimator=rf_pipe,\n",
    "                         estimatorParamMaps=rfParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)\n",
    "\n",
    "#cv = CrossValidator(estimator=random_forest_pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvmodel = crossval.fit(validation)\n",
    "cvmodel.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getMeasures(dataframe):\n",
    "#     Accuracy = MulticlassClassificationEvaluator(predictionCol = 'prediction', labelCol = 'gender_label', \\\n",
    "#                                                  metricName = 'accuracy').evaluate(dataframe)*100\n",
    "\n",
    "#     Precision = MulticlassClassificationEvaluator(predictionCol = 'prediction', labelCol = 'gender_label', \\\n",
    "#                                                  metricName = 'weightedPrecision').evaluate(dataframe)*100\n",
    "\n",
    "#     Recall = MulticlassClassificationEvaluator(predictionCol = 'prediction', labelCol = 'gender_label', \\\n",
    "#                                                  metricName = 'weightedRecall').evaluate(dataframe)*100\n",
    "\n",
    "#     F1 = MulticlassClassificationEvaluator(predictionCol = 'prediction', labelCol = 'gender_label', \\\n",
    "#                                                  metricName = 'f1').evaluate(dataframe)*100\n",
    "\n",
    "    \n",
    "#     TP = dataframe.select(\"gender_label\", \"prediction\").filter(\"gender_label = 0 and prediction = 0\").count\n",
    "#     TN = dataframe.select(\"gender_label\", \"prediction\").filter(\"gender_label = 1 and prediction = 1\").count\n",
    "#     FP = dataframe.select(\"gender_label\", \"prediction\").filter(\"gender_label = 0 and prediction = 1\").count\n",
    "#     FN = dataframe.select(\"gender_label\", \"prediction\").filter(\"gender_label = 1 and prediction = 0\").count\n",
    "#     total = dataframe.select(\"gender_label\").count\n",
    "    \n",
    "#     print(\"Accuracy: \", Accuracy)\n",
    "#     print(\"Precision: \", Precision)\n",
    "#     print(\"Recall: \", Recall)\n",
    "#     print(\"F1: \", F1)\n",
    "\n",
    "# # printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
